{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL BUILDING USING COSINE SIMILARITY"
      ],
      "metadata": {
        "id": "kvG3ZNAHr1p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "KN_aMYq8K5se"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.read_csv(\"/content/new_df.csv\")"
      ],
      "metadata": {
        "id": "ifZ8V_RLK8qp"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "CtLdaevpq6SY"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CountVectorizer_Scratch:\n",
        "    def __init__(self, max_features=None):\n",
        "        self.max_features = max_features\n",
        "        self.vocab = {}\n",
        "        self.vocabulary_ = {}\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        self.create_vocabulary(documents)\n",
        "        return self.transform(documents)\n",
        "\n",
        "    def create_vocabulary(self, documents):\n",
        "        word_count = {}\n",
        "        for document in documents:\n",
        "            for word in document.split():\n",
        "                if word not in word_count:\n",
        "                    word_count[word] = 1\n",
        "                else:\n",
        "                    word_count[word] += 1\n",
        "        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "        if self.max_features is not None:\n",
        "            sorted_words = sorted_words[:self.max_features]\n",
        "        self.vocabulary_ = {word[0]: index for index, word in enumerate(sorted_words)}\n",
        "\n",
        "    def transform(self, documents):\n",
        "        document_vectors = []\n",
        "        for document in documents:\n",
        "            vector = [0] * len(self.vocabulary_)\n",
        "            for word in document.split():\n",
        "                if word in self.vocabulary_:\n",
        "                    vector[self.vocabulary_[word]] += 1\n",
        "            document_vectors.append(vector)\n",
        "        return np.array(document_vectors)"
      ],
      "metadata": {
        "id": "XTTTb8oGRBzi"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer_Scratch(CountVectorizer_Scratch):\n",
        "    def __init__(self, max_features=None, stop_words=None):\n",
        "        super().__init__(max_features, stop_words)\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        count_matrix = super().fit_transform(documents)\n",
        "        tf_matrix = self.calculate_tf(count_matrix)\n",
        "        idf_vector = self.calculate_idf(count_matrix)\n",
        "        return self.calculate_tfidf(tf_matrix, idf_vector)\n",
        "\n",
        "    def calculate_tf(self, count_matrix):\n",
        "        tf_matrix = []\n",
        "        for row in count_matrix:\n",
        "            total_words = sum(row)\n",
        "            tf_vector = [count / total_words for count in row]\n",
        "            tf_matrix.append(tf_vector)\n",
        "        return tf_matrix\n",
        "\n",
        "    def calculate_idf(self, count_matrix):\n",
        "        idf_vector = []\n",
        "        total_documents = len(count_matrix)\n",
        "        for j in range(len(count_matrix[0])):\n",
        "            num_docs_containing_word = sum([1 for row in count_matrix if row[j] > 0])\n",
        "            idf = math.log(total_documents / (1 + num_docs_containing_word))\n",
        "            idf_vector.append(idf)\n",
        "        return idf_vector\n",
        "\n",
        "    def calculate_tfidf(self, tf_matrix, idf_vector):\n",
        "        tfidf_matrix = []\n",
        "        for tf_vector in tf_matrix:\n",
        "            tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "            tfidf_matrix.append(tfidf_vector)\n",
        "        return tfidf_matrix"
      ],
      "metadata": {
        "id": "0F3AtUhdS1Gf"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity_matrix(vectors):\n",
        "    num_vectors = len(vectors)\n",
        "    similarity_matrix = np.zeros((num_vectors, num_vectors))\n",
        "    for i in range(num_vectors):\n",
        "        for j in range(num_vectors):\n",
        "            similarity_matrix[i][j] = cosine_similarity_scratch(vectors[i], vectors[j])\n",
        "    return similarity_matrix\n",
        "\n",
        "def cosine_similarity_scratch(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    magnitude1 = math.sqrt(np.sum(np.square(vector1)))\n",
        "    magnitude2 = math.sqrt(np.sum(np.square(vector2)))\n",
        "\n",
        "    if magnitude1 == 0 or magnitude2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (magnitude1 * magnitude2)"
      ],
      "metadata": {
        "id": "V5ExS8U-R09Z"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "cv = CountVectorizer_Scratch(max_features=5000)\n",
        "tfid = TfidfVectorizer(max_features=1000)"
      ],
      "metadata": {
        "id": "_MCvnkQKryh4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = cv.fit_transform(new_df['transformed_text'])"
      ],
      "metadata": {
        "id": "Ssawz5QHr_vM"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L07Nfq7Gvcgb",
        "outputId": "6bad51ab-c9ae-48bb-c563-620c62cd29c9"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9742, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = cosine_similarity(vector)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbiyorKXX0Z4",
        "outputId": "6837df53-555f-4068-b9c5-5e4b98141200"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.63245553 0.28867513 ... 0.         0.13363062 0.11785113]\n",
            " [0.63245553 1.         0.18257419 ... 0.         0.         0.        ]\n",
            " [0.28867513 0.18257419 1.         ... 0.         0.         0.13608276]\n",
            " ...\n",
            " [0.         0.         0.         ... 1.         0.         0.        ]\n",
            " [0.13363062 0.         0.         ... 0.         1.         0.        ]\n",
            " [0.11785113 0.         0.13608276 ... 0.         0.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFZKr1e4svmL",
        "outputId": "72b7721a-f059-4ffe-b5cf-82109f6a3071"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9742, 9742)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(movie):\n",
        "    index = new_df[new_df['title'] == movie].index[0]\n",
        "    distances = sorted(list(enumerate(similarity[index])),reverse=True,key = lambda x: x[1])\n",
        "    for i in distances[1:6]:\n",
        "        print(new_df.iloc[i[0]].title)"
      ],
      "metadata": {
        "id": "q2Z2sn_isLYQ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend(\"Toy Story (1995)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuVr9xLdsUUs",
        "outputId": "0442b430-1380-4755-cfe7-18ee7eb588ca"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toy Story 2 (1999)\n",
            "Toy Story 3 (2010)\n",
            "Moana (2016)\n",
            "Antz (1998)\n",
            "Valiant (2005)\n"
          ]
        }
      ]
    }
  ]
}